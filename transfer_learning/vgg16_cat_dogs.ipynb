{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of BERT.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/blade1780/bert/blob/master/BERT.ipynb","timestamp":1564937350304}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"zFxy9VfywWj0","colab_type":"code","colab":{}},"source":["# coding: utf-8\n","\n","# https://github.com/google-research/bert\n","# https://github.com/CyberZHG/keras-bert\n","\n","# папка, куда распаковать преодобученную нейросеть BERT\n","folder = 'multi_cased_L-12_H-768_A-12'\n","download_url = 'https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip'  # ссылка на скачивание модели\n","\n","print('Downloading model...')\n","zip_path = '{}.zip'.format(folder)\n","!test -d $folder || (wget $download_url && unzip $zip_path)\n","\n","# скачиваем из BERT репозитория файл tokenization.py\n","!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n","\n","# install Keras BERT\n","!pip install keras-bert\n","\n","import sys\n","import numpy as np\n","from keras_bert import load_trained_model_from_checkpoint\n","import tokenization\n","\n","config_path = folder+'/bert_config.json'\n","checkpoint_path = folder+'/bert_model.ckpt'\n","vocab_path = folder+'/vocab.txt'\n","\n","# создаем объект для перевода строки с пробелами в токены\n","tokenizer = tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=False)\n","\n","# загружаем модель\n","print('Loading model...')\n","model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True)\n","#model.summary()          # информация о слоях нейросети - количество параметров и т.д.\n","print('OK')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zikPBVg8wkjZ","colab_type":"code","colab":{}},"source":["\n","# РЕЖИМ 1: предсказание слов, закрытых токеном MASK в фразе. На вход нейросети надо подать фразу в формате: [CLS] Я пришел в [MASK] и купил [MASK]. [SEP]\n","\n","# входная фраза с закрытыми словами с помощью [MASK]\n","sentence = 'Я пришел в [MASK] и купил [MASK].'  #@param {type:\"string\"}\n","\n","print(sentence)\n","\n","\n","#-------------------------\n","# преобразование в токены (tokenizer.tokenize() не обрабатывает [CLS], [MASK], поэтому добавим их вручную)\n","sentence = sentence.replace(' [MASK] ','[MASK]'); sentence = sentence.replace('[MASK] ','[MASK]'); sentence = sentence.replace(' [MASK]','[MASK]')  # удаляем лишние пробелы\n","sentence = sentence.split('[MASK]')             # разбиваем строку по маске\n","tokens = ['[CLS]']                              # фраза всегда должна начинаться на [CLS]\n","# обычные строки преобразуем в токены с помощью tokenizer.tokenize(), вставляя между ними [MASK]\n","for i in range(len(sentence)):\n","    if i == 0:\n","        tokens = tokens + tokenizer.tokenize(sentence[i]) \n","    else:\n","        tokens = tokens + ['[MASK]'] + tokenizer.tokenize(sentence[i]) \n","tokens = tokens + ['[SEP]']                     # фраза всегда должна заканчиваться на [SEP] \n","# в tokens теперь токены, которые гарантированно по словарю преобразуются в индексы\n","#-------------------------\n","#print(tokens)\n","\n","# преобразуем в массив индексов, который можно подавать на вход сети, причем число 103 в нем это [MASK]\n","token_input = tokenizer.convert_tokens_to_ids(tokens)        \n","#print(token_input)\n","# удлиняем до 512 длины\n","token_input = token_input + [0] * (512 - len(token_input))\n","\n","\n","# создаем маску, заменив все числа 103 на 1, а остальное 0\n","mask_input = [0]*512\n","for i in range(len(mask_input)):\n","    if token_input[i] == 103:\n","        mask_input[i] = 1\n","#print(mask_input)\n","\n","# маска фраз (вторая фраза маскируется числом 1, а все остальное числом 0)\n","seg_input = [0]*512\n","\n","\n","# конвертируем в numpy в форму (1,) -> (1,512)\n","token_input = np.asarray([token_input])\n","mask_input = np.asarray([mask_input])\n","seg_input = np.asarray([seg_input])\n","\n","\n","# пропускаем через нейросеть...\n","predicts = model.predict([token_input, seg_input, mask_input])[0]       # в [0] полная фраза с заполненными предсказанными словами на месте [MASK]\n","predicts = np.argmax(predicts, axis=-1)\n","\n","\n","# форматируем результат в строку, разделенную пробелами\n","predicts = predicts[0][:len(tokens)]    # длиной как исходная фраза (чтобы отсечь случайные выбросы среди нулей дальше)\n","out = []\n","# добавляем в out только слова в позиции [MASK], которые маскированы цифрой 1 в mask_input\n","for i in range(len(mask_input[0])):\n","    if mask_input[0][i] == 1:           # [0][i], т.к. требование было (1,512)\n","        out.append(predicts[i]) \n","\n","out = tokenizer.convert_ids_to_tokens(out)      # индексы в токены\n","out = ' '.join(out)                             # объединяем в одну строку с пробелами\n","out = tokenization.printable_text(out)          # в читабельную версию\n","out = out.replace(' ##','')                     # объединяем раздъединенные слова \"при ##шел\" -> \"пришел\"\n","print('Result:', out)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-R88pTmh6g-","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"FjXLJupvwoFx","colab_type":"code","colab":{}},"source":["\n","# РЕЖИМ 2: проверка логичности двух фраз. На вход нейросети надо подать фразу в формате: [CLS] Я пришел в магазин. [SEP] И купил молоко. [SEP]\n","\n","sentence_1 = 'Я пришел в магазин.'      #@param {type:\"string\"}\n","sentence_2 = 'И купил молоко.'          #@param {type:\"string\"}\n","\n","\n","print(sentence_1, '->', sentence_2)\n","\n","# строки в массивы токенов\n","tokens_sen_1 = tokenizer.tokenize(sentence_1)\n","tokens_sen_2 = tokenizer.tokenize(sentence_2)\n","\n","tokens = ['[CLS]'] + tokens_sen_1 + ['[SEP]'] + tokens_sen_2 + ['[SEP]']\n","#print(tokens)\n","\n","# преобразуем строковые токены в числовые индексы:\n","token_input = tokenizer.convert_tokens_to_ids(tokens)  \n","# удлиняем до 512      \n","token_input = token_input + [0] * (512 - len(token_input))\n","\n","# маска в этом режиме все 0\n","mask_input = [0] * 512\n","\n","# в маске предложений под второй фразой, включая конечный SEP, надо поставить 1, а все остальное заполнить 0\n","seg_input = [0]*512\n","len_1 = len(tokens_sen_1) + 2                   # длина первой фразы, +2 - включая начальный CLS и разделитель SEP\n","for i in range(len(tokens_sen_2)+1):            # +1, т.к. включая последний SEP\n","        seg_input[len_1 + i] = 1                # маскируем вторую фразу, включая последний SEP, единицами\n","#print(seg_input)\n","\n","\n","# конвертируем в numpy в форму (1,) -> (1,512)\n","token_input = np.asarray([token_input])\n","mask_input = np.asarray([mask_input])\n","seg_input = np.asarray([seg_input])\n","\n","\n","# пропускаем через нейросеть...\n","predicts = model.predict([token_input, seg_input, mask_input])[1]       # в [1] ответ на вопрос, является ли второе предложение логичным по смыслу\n","#print('Sentence is okey: ', not bool(np.argmax(predicts, axis=-1)[0]), predicts)\n","print('Sentence is okey:', int(round(predicts[0][0]*100)), '%')                    # [[0.9657724  0.03422766]] - левое число вероятность что второе предложение подходит по смыслу, а правое - что второе предложение случайное\n","out = int(round(predicts[0][0]*100)) \n"],"execution_count":0,"outputs":[]}]}
