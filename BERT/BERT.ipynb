{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of BERT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFxy9VfywWj0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71c2f5ef-a46f-4742-8636-8edd708a5c38"
      },
      "source": [
        "# coding: utf-8\n",
        "\n",
        "# https://github.com/google-research/bert\n",
        "# https://github.com/CyberZHG/keras-bert\n",
        "\n",
        "# папка, куда распаковать преодобученную нейросеть BERT\n",
        "folder = 'multi_cased_L-12_H-768_A-12'\n",
        "download_url = 'https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip'  # ссылка на скачивание модели\n",
        "\n",
        "print('Downloading model...')\n",
        "zip_path = '{}.zip'.format(folder)\n",
        "!test -d $folder || (wget $download_url && unzip $zip_path)\n",
        "\n",
        "# скачиваем из BERT репозитория файл tokenization.py\n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n",
        "\n",
        "# install Keras BERT\n",
        "!pip install keras-bert\n",
        "\n",
        "import sys\n",
        "import numpy as np\n",
        "from keras_bert import load_trained_model_from_checkpoint\n",
        "import tokenization\n",
        "\n",
        "config_path = folder+'/bert_config.json'\n",
        "checkpoint_path = folder+'/bert_model.ckpt'\n",
        "vocab_path = folder+'/vocab.txt'\n",
        "\n",
        "# создаем объект для перевода строки с пробелами в токены\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=vocab_path, do_lower_case=False)\n",
        "\n",
        "# загружаем модель\n",
        "print('Loading model...')\n",
        "model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True)\n",
        "#model.summary()          # информация о слоях нейросети - количество параметров и т.д.\n",
        "print('OK')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading model...\n",
            "--2019-08-04 17:05:33--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.197.176, 2404:6800:4004:808::2010\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.197.176|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 662903077 (632M) [application/zip]\n",
            "Saving to: ‘multi_cased_L-12_H-768_A-12.zip’\n",
            "\n",
            "multi_cased_L-12_H- 100%[===================>] 632.19M  73.8MB/s    in 9.4s    \n",
            "\n",
            "2019-08-04 17:05:43 (67.2 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip’ saved [662903077/662903077]\n",
            "\n",
            "Archive:  multi_cased_L-12_H-768_A-12.zip\n",
            "   creating: multi_cased_L-12_H-768_A-12/\n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.meta  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/vocab.txt  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_model.ckpt.index  \n",
            "  inflating: multi_cased_L-12_H-768_A-12/bert_config.json  \n",
            "--2019-08-04 17:05:52--  https://raw.githubusercontent.com/google-research/bert/master/tokenization.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12257 (12K) [text/plain]\n",
            "Saving to: ‘tokenization.py’\n",
            "\n",
            "tokenization.py     100%[===================>]  11.97K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-08-04 17:05:52 (136 MB/s) - ‘tokenization.py’ saved [12257/12257]\n",
            "\n",
            "Collecting keras-bert\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/61/fc2707e7e05aeb8876167ed96938f33933e5c480668ae8268c9a7572b51d/keras-bert-0.71.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.16.4)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-bert) (2.2.4)\n",
            "Collecting keras-transformer>=0.29.0 (from keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/9c/9dda9466a97cfd21300736555a257429a162d774cc4470a1b880796eaeac/keras-transformer-0.29.0.tar.gz\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-bert) (3.13)\n",
            "Collecting keras-pos-embd>=0.10.0 (from keras-transformer>=0.29.0->keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.20.0 (from keras-transformer>=0.29.0->keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/0f/f1a66974db9c328ba675c1df63f8a68c5c0f3e181f1e74db4f3b1a72a6df/keras-multi-head-0.20.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.12.0 (from keras-transformer>=0.29.0->keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/95/76/42878fe46bff8458d8aa1da50bfdf705d632d33dbb7b60db52a06faf2dad/keras-layer-normalization-0.12.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.5.0 (from keras-transformer>=0.29.0->keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.7.0 (from keras-transformer>=0.29.0->keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n",
            "Collecting keras-self-attention==0.41.0 (from keras-multi-head>=0.20.0->keras-transformer>=0.29.0->keras-bert)\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.71.0-cp36-none-any.whl size=33991 sha256=ade4fee937cf169aeb550ffff029188c1c633caed6016817a5902b6496c56cee\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/21/e2/219589c9dfe487c3b9f85b2097c96127f794c15e3bc87fc597\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.29.0-cp36-none-any.whl size=13392 sha256=4e9a5eb02aacd6364aa80ba9925a86e993b3d0b4dc1c01515d87f6f01264df38\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/77/37/8fcad8efc82751342b5e19134629444394a316cee7e0955f4d\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7553 sha256=c80bbe4286a070d8f2427ea8ad8488b8504bddc3ffcfb4374fb3cb1589027f8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.20.0-cp36-none-any.whl size=15360 sha256=af9bf0c66da757f124e9d1335bcba876ea736d70376024b8541efed3949992f2\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/49/02/4eda210bc4c37ff1d45311665bceb790881dbea92b27b025a5\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.12.0-cp36-none-any.whl size=5208 sha256=f2a9dec46582fb99b8c9d571887d0cd6cfedcaac395f04a0751976a815f340f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/c3/9b/9e/f4072915f660e90bb3638332276f4de80476f3afcb5d010d6f\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5623 sha256=c9a4926dfb7efa6a257df3322f6f8119bee5327fa88b2e3934cb3ec8ba4792e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4677 sha256=832fa0824d290a94c2ceb7c6e0dc840e5a4762770e5a90fd7fdb5afa5c9bdaa9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17289 sha256=ba2dff1b86a3941652c3938232f7cfdf3b6d7e33d38a79bc1e9564cee6a8a6f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n",
            "Successfully installed keras-bert-0.71.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.12.0 keras-multi-head-0.20.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.29.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0804 17:06:08.377993 139723878528896 deprecation_wrapper.py:119] From /content/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0804 17:06:08.783774 139723878528896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0804 17:06:08.820506 139723878528896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0804 17:06:08.871888 139723878528896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:131: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0804 17:06:08.872968 139723878528896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0804 17:06:08.887548 139723878528896 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0804 17:06:08.916900 139723878528896 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4185: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model...\n",
            "OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWTusNN4nPYT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zikPBVg8wkjZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "02ab530b-07b9-4fa1-fa28-63f3354569f6"
      },
      "source": [
        "\n",
        "# РЕЖИМ 1: предсказание слов, закрытых токеном MASK в фразе. На вход нейросети надо подать фразу в формате: [CLS] Я пришел в [MASK] и купил [MASK]. [SEP]\n",
        "\n",
        "# входная фраза с закрытыми словами с помощью [MASK]\n",
        "sentence = 'Я помню чудное мгновенье. Передо мной явилась [MASK]. Я пришёл в [MASK] и купил [MASK].'  #@param {type:\"string\"}\n",
        "\n",
        "print(sentence)\n",
        "\n",
        "\n",
        "#-------------------------\n",
        "# преобразование в токены (tokenizer.tokenize() не обрабатывает [CLS], [MASK], поэтому добавим их вручную)\n",
        "sentence = sentence.replace(' [MASK] ','[MASK]'); sentence = sentence.replace('[MASK] ','[MASK]'); sentence = sentence.replace(' [MASK]','[MASK]')  # удаляем лишние пробелы\n",
        "sentence = sentence.split('[MASK]')             # разбиваем строку по маске\n",
        "tokens = ['[CLS]']                              # фраза всегда должна начинаться на [CLS]\n",
        "# обычные строки преобразуем в токены с помощью tokenizer.tokenize(), вставляя между ними [MASK]\n",
        "for i in range(len(sentence)):\n",
        "    if i == 0:\n",
        "        tokens = tokens + tokenizer.tokenize(sentence[i]) \n",
        "    else:\n",
        "        tokens = tokens + ['[MASK]'] + tokenizer.tokenize(sentence[i]) \n",
        "tokens = tokens + ['[SEP]']                     # фраза всегда должна заканчиваться на [SEP] \n",
        "# в tokens теперь токены, которые гарантированно по словарю преобразуются в индексы\n",
        "#-------------------------\n",
        "#print(tokens)\n",
        "\n",
        "# преобразуем в массив индексов, который можно подавать на вход сети, причем число 103 в нем это [MASK]\n",
        "token_input = tokenizer.convert_tokens_to_ids(tokens)        \n",
        "#print(token_input)\n",
        "# удлиняем до 512 длины\n",
        "token_input = token_input + [0] * (512 - len(token_input))\n",
        "\n",
        "\n",
        "# создаем маску, заменив все числа 103 на 1, а остальное 0\n",
        "mask_input = [0]*512\n",
        "for i in range(len(mask_input)):\n",
        "    if token_input[i] == 103:\n",
        "        mask_input[i] = 1\n",
        "#print(mask_input)\n",
        "\n",
        "# маска фраз (вторая фраза маскируется числом 1, а все остальное числом 0)\n",
        "seg_input = [0]*512\n",
        "\n",
        "\n",
        "# конвертируем в numpy в форму (1,) -> (1,512)\n",
        "token_input = np.asarray([token_input])\n",
        "mask_input = np.asarray([mask_input])\n",
        "seg_input = np.asarray([seg_input])\n",
        "\n",
        "\n",
        "# пропускаем через нейросеть...\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[0]       # в [0] полная фраза с заполненными предсказанными словами на месте [MASK]\n",
        "predicts = np.argmax(predicts, axis=-1)\n",
        "\n",
        "\n",
        "# форматируем результат в строку, разделенную пробелами\n",
        "predicts = predicts[0][:len(tokens)]    # длиной как исходная фраза (чтобы отсечь случайные выбросы среди нулей дальше)\n",
        "out = []\n",
        "# добавляем в out только слова в позиции [MASK], которые маскированы цифрой 1 в mask_input\n",
        "for i in range(len(mask_input[0])):\n",
        "    if mask_input[0][i] == 1:           # [0][i], т.к. требование было (1,512)\n",
        "        out.append(predicts[i]) \n",
        "\n",
        "out = tokenizer.convert_ids_to_tokens(out)      # индексы в токены\n",
        "out = ' '.join(out)                             # объединяем в одну строку с пробелами\n",
        "out = tokenization.printable_text(out)          # в читабельную версию\n",
        "out = out.replace(' ##','')                     # объединяем раздъединенные слова \"при ##шел\" -> \"пришел\"\n",
        "print('Result:', out)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Я помню чудное мгновенье. Передо мной явилась [MASK]. Я пришёл в [MASK] и купил [MASK].\n",
            "Result: я дом её\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-R88pTmh6g-",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjXLJupvwoFx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "65c9cb23-4a7f-4302-896e-3501e4431080"
      },
      "source": [
        "\n",
        "# РЕЖИМ 2: проверка логичности двух фраз. На вход нейросети надо подать фразу в формате: [CLS] Я пришел в магазин. [SEP] И купил молоко. [SEP]\n",
        "\n",
        "sentence_1 = 'Я пришел в магазин.'      #@param {type:\"string\"}\n",
        "sentence_2 = 'И купил молоко.'          #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "print(sentence_1, '->', sentence_2)\n",
        "\n",
        "# строки в массивы токенов\n",
        "tokens_sen_1 = tokenizer.tokenize(sentence_1)\n",
        "tokens_sen_2 = tokenizer.tokenize(sentence_2)\n",
        "\n",
        "tokens = ['[CLS]'] + tokens_sen_1 + ['[SEP]'] + tokens_sen_2 + ['[SEP]']\n",
        "#print(tokens)\n",
        "\n",
        "# преобразуем строковые токены в числовые индексы:\n",
        "token_input = tokenizer.convert_tokens_to_ids(tokens)  \n",
        "# удлиняем до 512      \n",
        "token_input = token_input + [0] * (512 - len(token_input))\n",
        "\n",
        "# маска в этом режиме все 0\n",
        "mask_input = [0] * 512\n",
        "\n",
        "# в маске предложений под второй фразой, включая конечный SEP, надо поставить 1, а все остальное заполнить 0\n",
        "seg_input = [0]*512\n",
        "len_1 = len(tokens_sen_1) + 2                   # длина первой фразы, +2 - включая начальный CLS и разделитель SEP\n",
        "for i in range(len(tokens_sen_2)+1):            # +1, т.к. включая последний SEP\n",
        "        seg_input[len_1 + i] = 1                # маскируем вторую фразу, включая последний SEP, единицами\n",
        "#print(seg_input)\n",
        "\n",
        "\n",
        "# конвертируем в numpy в форму (1,) -> (1,512)\n",
        "token_input = np.asarray([token_input])\n",
        "mask_input = np.asarray([mask_input])\n",
        "seg_input = np.asarray([seg_input])\n",
        "\n",
        "\n",
        "# пропускаем через нейросеть...\n",
        "predicts = model.predict([token_input, seg_input, mask_input])[1]       # в [1] ответ на вопрос, является ли второе предложение логичным по смыслу\n",
        "#print('Sentence is okey: ', not bool(np.argmax(predicts, axis=-1)[0]), predicts)\n",
        "print('Sentence is okey:', int(round(predicts[0][0]*100)), '%')                    # [[0.9657724  0.03422766]] - левое число вероятность что второе предложение подходит по смыслу, а правое - что второе предложение случайное\n",
        "out = int(round(predicts[0][0]*100)) \n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Я пришел в магазин. -> И купил молоко.\n",
            "Sentence is okey: 99 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}